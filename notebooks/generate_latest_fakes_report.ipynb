{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import dotenv\n",
    "import io\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from IPython.display import Markdown, display\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "WORK = os.environ[\"WORK\"]\n",
    "PROJ_DIR = f'{WORK}/ADA_Project'\n",
    "\n",
    "def generate_latest_fakes_report(PROJ_DIR, verbose=False, export=False, display_output=False):\n",
    "\n",
    "    tr = f'{WORK}/ADA_Project/training_runs'\n",
    "\n",
    "    fid_logs = {}\n",
    "\n",
    "    tf_folders = glob(f'{tr}/*')\n",
    "    for f in tf_folders:\n",
    "        folder = sorted(glob(f'{f}/*'))[-1]\n",
    "        fid_file = glob(f'{folder}/metric-*.txt')\n",
    "        if fid_file != []:\n",
    "            fid_file = fid_file[0]\n",
    "            dataset = Path(fid_file).parents[1].name.replace(\n",
    "                '_training-runs', '')\n",
    "            fid_logs[dataset] = fid_file\n",
    "\n",
    "    fid_logs['AFHQ-CAT'] = fid_logs.pop('AFHQ')\n",
    "\n",
    "    fid_logs = {k.replace('FFHQ', 'FFHQ_custom'): v for k, v in fid_logs.items()}\n",
    "\n",
    "    findWholeWord = lambda w, s: re.compile(rf'\\b({w})\\b', flags=re.IGNORECASE\n",
    "                                            ).search(s)\n",
    "\n",
    "    snapshots = {}\n",
    "\n",
    "    for k, v in fid_logs.items():\n",
    "        with open(v) as f:\n",
    "            lines = f.readlines()\n",
    "            snapshots[k] = {}\n",
    "            snapshots[k]['scores'] = []\n",
    "            for line in lines:\n",
    "                if 'StyleGAN2' in k:\n",
    "                    string = 'fid50k'\n",
    "                else:\n",
    "                    string = 'fid50k_full'\n",
    "                sp = findWholeWord(string, line).span()\n",
    "                snapshot = line[:23]\n",
    "                score = float(line[sp[-1] + 1:sp[-1] + 7])\n",
    "                snapshots[k]['scores'].append({f'{snapshot}': score})\n",
    "\n",
    "    best_snapshots = {}\n",
    "\n",
    "    for ds in snapshots:\n",
    "        d = snapshots[ds]['scores']\n",
    "        keys = [list(x.keys()) for x in d]\n",
    "        vals = [list(x.values()) for x in d]\n",
    "        best_snapshots[ds] = {\n",
    "            'snapshot': keys[vals.index(min(vals))][0],\n",
    "            'score': min(vals)[0]\n",
    "        }\n",
    "\n",
    "    files = [v.replace('metric-fid50k_full.txt', 'log.txt').replace(\n",
    "        'metric-fid50k.txt', 'log.txt')\n",
    "             for k, v in fid_logs.items()]\n",
    "\n",
    "    for (k, v), f in zip(best_snapshots.items(), files):\n",
    "        best_snapshots[k]['file'] = f.replace(f'{tr}/' , '')\n",
    "\n",
    "    if export is True:\n",
    "        with open(f'{PROJ_DIR}/FID_of_best_snapshots.json', 'w') as out_file:\n",
    "            json.dump(best_snapshots, out_file, indent=4)\n",
    "\n",
    "            \n",
    "    d = best_snapshots\n",
    "\n",
    "    for k, v in best_snapshots.items():\n",
    "        d[k]['training_time'] = []\n",
    "\n",
    "\n",
    "    findWholeWord = lambda w, s: re.compile(rf'\\b({w})\\b', flags=re.IGNORECASE\n",
    "                                            ).search(s)\n",
    "\n",
    "    def calc_time(t, unit):\n",
    "        s = t.partition(unit)[0][-2:].replace(' ', '')\n",
    "        if t.partition(unit)[0] != t:\n",
    "            return int(s)\n",
    "        return 0\n",
    "\n",
    "    TTs = {}\n",
    "\n",
    "    for k, v in d.items():\n",
    "        file = f'{tr}/{d[k][\"file\"]}'\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        if 'Exporting sample images...' in lines[-1]:\n",
    "            continue\n",
    "        else:\n",
    "            snap = f'{tr}/{d[k][\"snapshot\"]}'\n",
    "            snap = Path(snap).name\n",
    "            for line in lines:\n",
    "                if snap in line:\n",
    "                    line_idx = lines.index(line) - 2\n",
    "                    line = lines[line_idx]\n",
    "                    try:\n",
    "                        sp = findWholeWord('time', line).span()\n",
    "                        t = line[sp[1] + 1:sp[1] + 12]\n",
    "                        last = t.partition('s')[-1]\n",
    "                        t = t.replace(last, '')\n",
    "                        T = (calc_time(t, 'd') * 24) + calc_time(t, 'h') + (\n",
    "                            calc_time(t, 'm') / 60) + (calc_time(t, 's') / 3600)\n",
    "                        d[k]['training_time'].append(T)\n",
    "\n",
    "                    except AttributeError:\n",
    "                        continue\n",
    "\n",
    "    days = [round(j['training_time'][0] / 24, 1) for i, j in d.items()]\n",
    "\n",
    "    table = tabulate([[x, round(y['training_time'][0], 2), z, round(y['score'], 2)]\n",
    "                      for (x, y), z in zip(d.items(), days)],\n",
    "                     headers=[\n",
    "                         'Dataset', 'Training time (in hrs)',\n",
    "                         'Training time (in days)', 'FID'\n",
    "                     ],\n",
    "                     tablefmt='github')\n",
    "\n",
    "\n",
    "\n",
    "    def upload_img(image, token):\n",
    "        with open(image, \"rb\") as file:\n",
    "            url = \"https://api.imgbb.com/1/upload\"\n",
    "            parameters = {\n",
    "                \"key\": token,\n",
    "                \"image\": base64.b64encode(file.read()),\n",
    "            }\n",
    "            res = requests.post(url, parameters)\n",
    "            link = res.json()\n",
    "            url = link['data']['url']\n",
    "            return url\n",
    "\n",
    "    dotenv.load_dotenv(f'{PROJ_DIR}/.env')\n",
    "    token = os.getenv('TOKEN')\n",
    "\n",
    "    mb_size = lambda x: Path(x).stat().st_size / (1024 * 1024)\n",
    "    dir_up = lambda x, y: \"/\".join(Path(x).parts[y:])\n",
    "\n",
    "    TRfolders_ = f'{PROJ_DIR}/training_runs'\n",
    "    TRfolders = glob(f'{TRfolders_}/*')\n",
    "    backups_dir = f'{PROJ_DIR}/.tmp_imgs'\n",
    "    Path(backups_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    md_content = []\n",
    "    latest_fakes = [str(Path(tr + '/' + d[k][\"file\"]).parent) +\n",
    "                    f'/{d[k][\"snapshot\"]}.png'.replace('network-snapshot-', 'fakes')\n",
    "                    for k, v in d.items()]\n",
    "\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime('%m/%d/%Y, %H:%M:%S')\n",
    "    md_content.append('# Latest fakes\\n')\n",
    "    md_content.append(f'## Date and time: {date_time}\\n')\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print('=' * 90, '\\n\\nLatest fakes:\\n')\n",
    "        pprint([x.replace(str(TRfolders_), '') for x in latest_fakes])\n",
    "        print('\\n', '=' * 90, '\\n')\n",
    "\n",
    "    for img in latest_fakes:\n",
    "        image = Image.open(img)\n",
    "        compressed_path = f'{backups_dir}/{Path(img).stem}' + '.jpg'\n",
    "        \n",
    "        if 'StyleGAN2_WILD-AFHQ' in img:\n",
    "            left, top, right, bottom = 0, 0, 256 * 15, 256 * 8\n",
    "            image = image.crop((left, top, right, bottom))\n",
    "            temp = io.BytesIO()\n",
    "            \n",
    "        image.save(compressed_path)\n",
    "            \n",
    "        if verbose:\n",
    "            print(\n",
    "                Path(img).name,\n",
    "                f'compressed from ({mb_size(img):.2f}MB) to ==> '\n",
    "                f'({mb_size(compressed_path):.2f}MB)')\n",
    "\n",
    "        url = upload_img(compressed_path, token)\n",
    "        if verbose == 1:\n",
    "            print(f'Link ==> {url}\\n')\n",
    "        img_subdir = dir_up(img, -3)\n",
    "\n",
    "        md_content.append(\n",
    "            f'### {img_subdir}\\n'\n",
    "            f'![{Path(compressed_path).name}]({url} \"{img_subdir}\")'\n",
    "            '\\n\\n')\n",
    "\n",
    "#     Tstamp = datetime.now().strftime('%m_%d_%Y__%H_%M')\n",
    "    report_path = f'{PROJ_DIR}/latest_fakes_report.md'\n",
    "\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(''.join(md_content))\n",
    "        f.write(table)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Generated a report at ==> {report_path}')\n",
    "\n",
    "    if display_output:\n",
    "        display(Markdown(report_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================== \n",
      "\n",
      "Latest fakes:\n",
      "\n",
      "['/AFHQ-WILD_training-runs/00006-AFHQ-WILD_custom-auto2-resumecustom/fakes010199.png',\n",
      " '/metfaces_training-runs/00006-metfaces_custom-auto2-resumecustom/fakes004112.png',\n",
      " '/POKEMON_training-runs/00004-POKEMON_custom-auto2-resumecustom/fakes010813.png',\n",
      " '/StanfordDogs_training-runs/00005-StanfordDogs_custom-auto2-resumecustom/fakes011919.png',\n",
      " '/StyleGAN2_AFHQ-DOG_training-runs/00003-stylegan2-AFHQ-DOG_custom-2gpu-config-f/fakes001209.png',\n",
      " '/cars196_training-runs/00014-cars196_custom-auto2-resumecustom/fakes001843.png',\n",
      " '/AFHQ-DOG_training-runs/00001-AFHQ-DOG_custom-auto2-resumecustom/fakes008560.png',\n",
      " '/FFHQ_training-runs_5K/00002-FFHQ_custom_5K-auto2-resumecustom/fakes013639.png',\n",
      " '/FFHQ_training-runs_2K/00001-FFHQ_custom_2K-auto2-resumecustom/fakes011796.png',\n",
      " '/ANIME-FACES_training-runs/00005-ANIME-FACES_custom-auto2-resumecustom/fakes001171.png',\n",
      " '/StyleGAN2_WILD-AFHQ_training-runs/00006-stylegan2-AFHQ-WILD_custom-2gpu-config-f/fakes001532.png',\n",
      " '/102flowers_training-runs/00027-102flowers_custom-auto2-resumecustom/fakes000245.png',\n",
      " '/FFHQ_training-runs_30K/00001-FFHQ_custom_30K-auto2-resumecustom/fakes012902.png',\n",
      " '/FFHQ_training-runs/00002-FFHQ_custom-auto2-resumecustom/fakes013516.png',\n",
      " '/AFHQ_training-runs/00004-AFHQ_custom-auto2-resumecustom/fakes000635.png']\n",
      "\n",
      " ========================================================================================== \n",
      "\n",
      "fakes010199.png compressed from (16.15MB) to ==> (1.83MB)\n",
      "Link ==> https://i.ibb.co/qj4YSDn/0b96e122f2ee.jpg\n",
      "\n",
      "fakes004112.png compressed from (53.23MB) to ==> (4.36MB)\n",
      "Link ==> https://i.ibb.co/P5cr6R8/8d14b1437dc9.jpg\n",
      "\n",
      "fakes010813.png compressed from (14.43MB) to ==> (1.56MB)\n",
      "Link ==> https://i.ibb.co/nCKHV6r/e8d637d1b262.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_latest_fakes_report(PROJ_DIR=PROJ_DIR, verbose=True, export=True, display_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ada-env)",
   "language": "python",
   "name": "ada-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
